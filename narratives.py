"""
Narrative detection and company trend detector for Stoncs.

Steps:
- Load headlines from Snowflake
- Preprocess text (lowercase, remove punctuation)
- Create embeddings (SentenceTransformers)
- Cluster embeddings with KMeans
- Extract narrative labels (top terms per cluster)
- Detect company/ticker mentions and compute trend scores per narrative
- Persist enriched results back to Snowflake for the dashboard/optimizer

This module uses local sklearn clustering for the hackathon demo. In a
production setting Snowpark-ML can replace sklearn and embeddings can be
generated by a cloud API and stored in Snowflake VECTOR columns.
"""
from typing import List, Dict
import re
import json

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans as SKLearnKMeans
from sklearn.feature_extraction.text import TfidfVectorizer

from .snowflake_api_client import run_query, upload_csv, authenticate
import os


def preprocess_text(s: str) -> str:
    """Lowercase and remove punctuation from a headline."""
    if not isinstance(s, str):
        return ""
    s = s.lower()
    # Remove punctuation (simple approach)
    s = re.sub(r"[^a-z0-9\s()%$]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def detect_narratives(schema_news: str = "STONCS_NEWS", n_clusters: int = 6):
    """End-to-end narrative detection pipeline.

    - Reads NEWS_HEADLINES from Snowflake
    - Preprocesses and embeds headlines
    - Runs KMeans on embeddings
    - Extracts narrative labels via TF-IDF top terms
    - Computes company trend scores per narrative
    - Writes two tables back to Snowflake:
      - STONCS_NEWS.NARRATIVES (cluster_id, label, size, top_terms)
      - STONCS_NEWS.NEWS_ENRICHED (published_date, headline, company, ticker, preprocessed, cluster_id, embedding)
    """
    # Use REST API to fetch headlines
    authenticate()
    news_table = f"{schema_news}.NEWS_HEADLINES"
    sql = f"SELECT published_date, headline, company, ticker FROM {news_table}"
    res = run_query(sql)
    rows = []
    data = res.get("data") or {}
    # Handle the common Snowflake REST response shape
    if isinstance(data, dict) and data.get("rowset"):
        rows = data.get("rowset")
    elif isinstance(res.get("data"), list):
        rows = res.get("data")
    if not rows:
        df = pd.DataFrame(columns=["published_date", "headline", "company", "ticker"])
    else:
        df = pd.DataFrame(rows, columns=["published_date", "headline", "company", "ticker"])

    # Preprocess
    df["preprocessed"] = df["headline"].apply(preprocess_text)

    # Build embeddings
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(df["preprocessed"].tolist(), show_progress_bar=True)

    # Ensure target tables exist (we store embeddings as VARIANT JSON via REST)
    narratives_table = f"{schema_news}.NARRATIVES"
    enriched_table = f"{schema_news}.NEWS_ENRICHED"
    trends_table = f"{schema_news}.COMPANY_TRENDS"

    run_query(f"CREATE TABLE IF NOT EXISTS {narratives_table} (cluster_id INT, label STRING, size INT, top_terms VARIANT)")
    run_query(f"CREATE TABLE IF NOT EXISTS {enriched_table} (published_date DATE, headline STRING, company STRING, ticker STRING, preprocessed STRING, cluster_id INT, embedding VARIANT)")
    run_query(f"CREATE TABLE IF NOT EXISTS {trends_table} (cluster_id INT, mentioned_token STRING, mentions INT)")

    # Default: use local sklearn KMeans for clustering (robust in demo env)
    kmeans = SKLearnKMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)
    df["cluster_id"] = labels

    # Narrative labels: top TF-IDF terms per cluster
    vectorizer = TfidfVectorizer(max_features=200, stop_words="english")
    X = vectorizer.fit_transform(df["preprocessed"].fillna(""))
    terms = np.array(vectorizer.get_feature_names_out())

    narratives = []
    for cid in range(n_clusters):
        idx = np.where(labels == cid)[0]
        size = len(idx)
        if size == 0:
            label = "(empty)"
            top_terms = []
        else:
            cluster_tfidf = np.asarray(X[idx].sum(axis=0)).ravel()
            top_idx = cluster_tfidf.argsort()[::-1][:10]
            top_terms = terms[top_idx].tolist()
            label = ", ".join(top_terms[:5])

        narratives.append({"cluster_id": int(cid), "label": label, "size": int(size), "top_terms": top_terms})

    narratives_df = pd.DataFrame(narratives)

    # Company trend: count mentions of each ticker/company per cluster
    def extract_company_tokens(row):
        if pd.notna(row.get("ticker")):
            return [row.get("ticker")]
        if pd.notna(row.get("company")):
            return [row.get("company")]
        return re.findall(r"\b[A-Z]{1,5}\b", row.get("headline", ""))

    df["mentioned_tokens"] = df.apply(extract_company_tokens, axis=1)
    exploded = df.explode("mentioned_tokens")
    exploded = exploded[exploded["mentioned_tokens"].notna()]
    trend_table = exploded.groupby(["cluster_id", "mentioned_tokens"]).size().reset_index(name="mentions")

    # Persist narratives and trends back to Snowflake via REST run_query (batched INSERTs)
    authenticate()

    def insert_rows(table: str, cols: list, rows_values: list, batch: int = 200):
        for i in range(0, len(rows_values), batch):
            batch_rows = rows_values[i : i + batch]
            vals = ", ".join(batch_rows)
            sql = f"INSERT INTO {table} ({', '.join(cols)}) VALUES {vals}"
            run_query(sql)
        return len(rows_values)

    # Narratives table
    run_query(f"TRUNCATE TABLE {narratives_table}")
    narratives_rows = []
    for _, r in narratives_df.iterrows():
        top_json = json.dumps(r["top_terms"]) if isinstance(r["top_terms"], list) else json.dumps([])
        narratives_rows.append(
            f"({int(r['cluster_id'])}, '{r['label'].replace("'", "''")}', {int(r['size'])}, parse_json('{top_json.replace("'", "''")}'))"
        )
    if narratives_rows:
        insert_rows(narratives_table, ["cluster_id", "label", "size", "top_terms"], narratives_rows)

    # Trends table
    run_query(f"TRUNCATE TABLE {trends_table}")
    trend_rows = []
    for _, r in trend_table.iterrows():
        trend_rows.append(f"({int(r['cluster_id'])}, '{str(r['mentioned_tokens']).replace("'", "''")}', {int(r['mentions'])})")
    if trend_rows:
        insert_rows(trends_table, ["cluster_id", "mentioned_token", "mentions"], trend_rows)

    # Enriched table with embeddings as VARIANT (JSON)
    run_query(f"TRUNCATE TABLE {enriched_table}")
    enriched_rows = []
    for idx, r in df[["published_date", "headline", "company", "ticker", "preprocessed", "cluster_id"]].iterrows():
        emb = embeddings[idx].tolist()
        emb_json = json.dumps(emb).replace("'", "''")
        headline = str(r["headline"]).replace("'", "''")
        company = str(r.get("company") or "").replace("'", "''")
        ticker = str(r.get("ticker") or "").replace("'", "''")
        pre = str(r.get("preprocessed") or "").replace("'", "''")
        pd_val = r.get("published_date")
        pd_str = pd_val if pd.isna(pd_val) else str(pd_val)
        enriched_rows.append(
            f"('{pd_str}', '{headline}', '{company}', '{ticker}', '{pre}', {int(r['cluster_id'])}, parse_json('{emb_json}'))"
        )
    if enriched_rows:
        insert_rows(enriched_table, ["published_date", "headline", "company", "ticker", "preprocessed", "cluster_id", "embedding"], enriched_rows)

    # Persist model centers
    models_table = f"{schema_news}.NARRATIVE_MODELS"
    run_query(f"CREATE TABLE IF NOT EXISTS {models_table} (cluster_id INT, center VARIANT)")
    run_query(f"TRUNCATE TABLE {models_table}")
    centers = kmeans.cluster_centers_.tolist()
    center_rows = [f"({i}, parse_json('{json.dumps(center).replace("'", "''")}'))" for i, center in enumerate(centers)]
    if center_rows:
        insert_rows(models_table, ["cluster_id", "center"], center_rows)

    return narratives_df, trend_table


if __name__ == "__main__":
    print("Run detect_narratives() to detect narratives and persist results to Snowflake.")
