"""
Narrative detection and company trend detector for Stoncs.

Steps:
- Load headlines from Snowflake
- Preprocess text (lowercase, remove punctuation)
- Create embeddings (SentenceTransformers)
- Cluster embeddings with KMeans
- Extract narrative labels (top terms per cluster)
- Detect company/ticker mentions and compute trend scores per narrative
- Persist enriched results back to Snowflake for the dashboard/optimizer

This module uses local sklearn clustering for the hackathon demo. In a
production setting Snowpark-ML can replace sklearn and embeddings can be
generated by a cloud API and stored in Snowflake VECTOR columns.
"""
from typing import List, Dict
import re
import json

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans as SKLearnKMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import importlib

# Robust loader for the Snowflake REST client
_sf = None
try:
    _sf = importlib.import_module("stoncs.snowflake_api_client")
except Exception:
    try:
        _sf = importlib.import_module("snowflake_api_client")
    except Exception:
        _sf = None

if _sf is not None:
    run_query = getattr(_sf, "run_query")
    upload_csv = getattr(_sf, "upload_csv")
    authenticate = getattr(_sf, "authenticate")
else:
    def _missing(*args, **kwargs):
        raise ImportError("snowflake_api_client is not available. Set SNOWFLAKE env vars or ensure the package is importable.")

    run_query = _missing
    upload_csv = _missing
    authenticate = _missing
import os


def preprocess_text(s: str) -> str:
    """Lowercase and remove punctuation from a headline."""
    if not isinstance(s, str):
        return ""
    s = s.lower()
    # Remove punctuation (simple approach)
    s = re.sub(r"[^a-z0-9\s()%$]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def detect_narratives(schema_news: str = "STONCS_NEWS", n_clusters: int = 6):
    """End-to-end narrative detection pipeline.

    - Reads NEWS_HEADLINES from Snowflake
    - Preprocesses and embeds headlines
    - Runs KMeans on embeddings
    - Extracts narrative labels via TF-IDF top terms
    - Computes company trend scores per narrative
    - Writes two tables back to Snowflake:
      - STONCS_NEWS.NARRATIVES (cluster_id, label, size, top_terms)
      - STONCS_NEWS.NEWS_ENRICHED (published_date, headline, company, ticker, preprocessed, cluster_id, embedding)
    """
    # Use REST API to fetch headlines
    authenticate()
    news_table = f"{schema_news}.NEWS_HEADLINES"
    sql = f"SELECT published_date, headline, company, ticker FROM {news_table}"
    res = run_query(sql)
    rows = []
    data = res.get("data") or {}
    # Handle the common Snowflake REST response shape
    if isinstance(data, dict) and data.get("rowset"):
        rows = data.get("rowset")
    elif isinstance(res.get("data"), list):
        rows = res.get("data")
    if not rows:
        df = pd.DataFrame(columns=["published_date", "headline", "company", "ticker"])
    else:
        df = pd.DataFrame(rows, columns=["published_date", "headline", "company", "ticker"])

    # Preprocess
    df["preprocessed"] = df["headline"].apply(preprocess_text)

    # Build embeddings
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(df["preprocessed"].tolist(), show_progress_bar=True)

    # Ensure target tables exist (we store embeddings as VARIANT JSON via REST)
    narratives_table = f"{schema_news}.NARRATIVES"
    enriched_table = f"{schema_news}.NEWS_ENRICHED"
    trends_table = f"{schema_news}.COMPANY_TRENDS"

    # Persist top terms as a simple STRING to avoid VARIANT/JSON parsing issues in batch INSERTs
    run_query(f"CREATE TABLE IF NOT EXISTS {narratives_table} (cluster_id INT, label STRING, size INT, top_terms STRING)")
    # Skip storing embeddings in NEWS_ENRICHED for now (would require VARIANT handling)
    run_query(f"CREATE TABLE IF NOT EXISTS {trends_table} (cluster_id INT, mentioned_token STRING, mentions INT)")

    # Default: use local sklearn KMeans for clustering (robust in demo env)
    kmeans = SKLearnKMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)
    df["cluster_id"] = labels

    # Narrative labels: top TF-IDF terms per cluster
    vectorizer = TfidfVectorizer(max_features=200, stop_words="english")
    X = vectorizer.fit_transform(df["preprocessed"].fillna(""))
    terms = np.array(vectorizer.get_feature_names_out())

    narratives = []
    for cid in range(n_clusters):
        idx = np.where(labels == cid)[0]
        size = len(idx)
        if size == 0:
            label = "(empty)"
            top_terms = []
        else:
            cluster_tfidf = np.asarray(X[idx].sum(axis=0)).ravel()
            top_idx = cluster_tfidf.argsort()[::-1][:10]
            top_terms = terms[top_idx].tolist()
            label = ", ".join(top_terms[:5])

        narratives.append({"cluster_id": int(cid), "label": label, "size": int(size), "top_terms": top_terms})

    narratives_df = pd.DataFrame(narratives)

    # Company trend: count mentions of each ticker/company per cluster
    def extract_company_tokens(row):
        if pd.notna(row.get("ticker")):
            return [row.get("ticker")]
        if pd.notna(row.get("company")):
            return [row.get("company")]
        return re.findall(r"\b[A-Z]{1,5}\b", row.get("headline", ""))

    df["mentioned_tokens"] = df.apply(extract_company_tokens, axis=1)
    exploded = df.explode("mentioned_tokens")
    exploded = exploded[exploded["mentioned_tokens"].notna()]
    trend_table = exploded.groupby(["cluster_id", "mentioned_tokens"]).size().reset_index(name="mentions")

    # Persist narratives and trends back to Snowflake via REST run_query (batched INSERTs)
    authenticate()

    def insert_rows_with_connector(table: str, cols: list, param_rows: list, batch: int = 200):
        """Insert rows using the Snowflake connector with parameterized executemany.

        param_rows: list of tuples matching cols. For JSON/VARIANT fields, pass JSON string and
        use parse_json(%s) in the statement.
        """
        try:
            import os
            import snowflake.connector as sfconn
        except Exception:
            # connector not available — fall back to run_query batched SQL (best-effort)
            for i in range(0, len(param_rows), batch):
                batch_rows = param_rows[i : i + batch]
                # Build VALUES expressions (escape single quotes)
                vals = []
                for row in batch_rows:
                    esc = []
                    for v in row:
                        if v is None:
                            esc.append('NULL')
                        else:
                            s = str(v).replace("'", "''")
                            esc.append(f"'{s}'")
                    vals.append(f"({', '.join(esc)})")
                sql = f"INSERT INTO {table} ({', '.join(cols)}) VALUES " + ",".join(vals)
                run_query(sql)
            return len(param_rows)

        # Connector is available — use executemany with placeholders.
        user = os.environ.get('SNOWFLAKE_USER')
        pwd = os.environ.get('SNOWFLAKE_PASSWORD')
        account = os.environ.get('SNOWFLAKE_ACCOUNT')
        if not (user and pwd and account):
            # fallback to run_query if creds missing
            for i in range(0, len(param_rows), batch):
                batch_rows = param_rows[i : i + batch]
                vals = []
                for row in batch_rows:
                    esc = []
                    for v in row:
                        if v is None:
                            esc.append('NULL')
                        else:
                            s = str(v).replace("'", "''")
                            esc.append(f"'{s}'")
                    vals.append(f"({', '.join(esc)})")
                sql = f"INSERT INTO {table} ({', '.join(cols)}) VALUES " + ",".join(vals)
                run_query(sql)
            return len(param_rows)

        conn = sfconn.connect(user=user, password=pwd, account=account)
        try:
            cur = conn.cursor()
            # Ensure session context uses provided database/schema/warehouse if present
            db = os.environ.get('SNOWFLAKE_DATABASE')
            schema_env = os.environ.get('SNOWFLAKE_SCHEMA')
            wh = os.environ.get('SNOWFLAKE_WAREHOUSE')
            if db:
                try:
                    cur.execute(f"USE DATABASE {db}")
                except Exception:
                    cur.execute(f'USE DATABASE "{db}"')
            if schema_env:
                try:
                    cur.execute(f"USE SCHEMA {schema_env}")
                except Exception:
                    cur.execute(f'USE SCHEMA "{schema_env}"')
            if wh:
                try:
                    cur.execute(f"USE WAREHOUSE {wh}")
                except Exception:
                    cur.execute(f'USE WAREHOUSE "{wh}"')

            # Use plain %s placeholders for all columns and let the Snowflake
            # connector convert Python lists/dicts to VARIANT automatically.
            placeholder = ','.join(['%s'] * len(cols))
            col_list = ', '.join(cols)
            sql = f"INSERT INTO {table} ({col_list}) VALUES ({placeholder})"

            inserted = 0
            for i in range(0, len(param_rows), batch):
                batch_rows = param_rows[i : i + batch]
                # Decide if any row contains Python lists/dicts (VARIANT fields)
                needs_per_row = False
                for row in batch_rows:
                    for v in row:
                        if isinstance(v, (list, dict)):
                            needs_per_row = True
                            break
                    if needs_per_row:
                        break

                if not needs_per_row:
                    # fast path
                    cur.executemany(sql, batch_rows)
                    inserted += len(batch_rows)
                else:
                    # Safe per-row insertion building SQL literals; use PARSE_JSON for VARIANT columns
                    for row in batch_rows:
                        literal_parts = []
                        for col_name, val in zip(cols, row):
                            if val is None:
                                literal_parts.append('NULL')
                                continue
                            if isinstance(val, (list, dict)):
                                # JSON encode and escape single quotes
                                j = json.dumps(val).replace("'", "''")
                                literal_parts.append(f"parse_json('{j}')")
                            elif isinstance(val, (int, float)):
                                literal_parts.append(str(val))
                            else:
                                s = str(val).replace("'", "''")
                                literal_parts.append(f"'{s}'")
                        vals_sql = ', '.join(literal_parts)
                        row_sql = f"INSERT INTO {table} ({', '.join(cols)}) VALUES ({vals_sql})"
                        cur.execute(row_sql)
                        inserted += 1
            conn.commit()
            cur.close()
            return inserted
        finally:
            try:
                conn.close()
            except Exception:
                pass

    # Narratives table
    run_query(f"TRUNCATE TABLE {narratives_table}")
    narratives_params = []
    for _, r in narratives_df.iterrows():
        # store top terms as a comma-joined string for portability
        tt = r.get("top_terms") if isinstance(r.get("top_terms"), list) else []
        tt_str = ", ".join(tt)
        narratives_params.append((int(r['cluster_id']), r['label'], int(r['size']), tt_str))
    if narratives_params:
        insert_rows_with_connector(narratives_table, ["cluster_id", "label", "size", "top_terms"], narratives_params)

    # Trends table
    run_query(f"TRUNCATE TABLE {trends_table}")
    trend_params = []
    for _, r in trend_table.iterrows():
        trend_params.append((int(r['cluster_id']), str(r['mentioned_tokens']), int(r['mentions'])))
    if trend_params:
        insert_rows_with_connector(trends_table, ["cluster_id", "mentioned_token", "mentions"], trend_params)

    # We skip persisting embeddings and model centers in this demo run to avoid
    # VARIANT/JSON handling edge cases. The important artifacts (narratives and
    # trend counts) have been persisted above.

    return narratives_df, trend_table


if __name__ == "__main__":
    print("Run detect_narratives() to detect narratives and persist results to Snowflake.")
